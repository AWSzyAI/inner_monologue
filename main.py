import pandas as pd
import re
import json
import logging
import os
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from kimi_api import send_messages, MODEL_NAME  # å¼•å…¥ MODEL_NAME

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename="generation_log.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è®¾å®šå¹¶å‘çº¿ç¨‹æ•° (æ ¹æ® API é€Ÿç‡è°ƒæ•´)
MAX_WORKERS = 5

CHECKPOINT_FILE = "checkpoint.txt"
CACHE_FILE = "cache.csv"
FAIL_FILE = "fail_data.csv"
OUTPUT_FILE = "è‡ªæˆ‘è‚¯å®šè¯­_ç”Ÿæˆæ—ç™½.csv"

def clean_value(value):
    """æ¸…ç†å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ¢è¡Œç¬¦ \n è¢«è½¬æ¢ä¸º 'å‰æ–‡\\nåæ–‡' å½¢å¼"""
    if isinstance(value, str):
        return value.replace("\n", "\\n")  # è½¬æ¢æ¢è¡Œç¬¦
    return value

def extract_json(response):
    """è§£æ AI å“åº”ï¼Œç¡®ä¿ JSON æ ¼å¼æ­£ç¡®"""
    match = re.search(r'\{.*\}', response, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))  # è§£æ JSON æ•°æ®
        except json.JSONDecodeError as e:
            logging.warning(f"JSON è§£æå¤±è´¥: {e}, è·³è¿‡è¯¥æ¡æ•°æ®: {response}")
            return None  # è§£æå¤±è´¥
    else:
        logging.warning(f"æœªæ‰¾åˆ° JSON æ•°æ®: {response}")
        return None  # æœªæ‰¾åˆ° JSON

def process_sentence(index, sentence):
    """å¤„ç†å•ä¸ªè‡ªæˆ‘è‚¯å®šè¯­ï¼Œç”Ÿæˆå†…å¿ƒæ—ç™½"""
    prompt = f"""
    è‡ªæˆ‘è‚¯å®šè¯­ï¼š{sentence}

    è¯·ä»¿ç…§è¨æäºšçš„ã€Šå½“æˆ‘çœŸçš„æ„¿æ„çœ‹è§è‡ªå·±æ—¶ã€‹çš„é£æ ¼ï¼Œä¸ºè¾“å…¥çš„è‡ªæˆ‘è‚¯å®šè¯­ç”Ÿæˆä¸€æ®µå†…å¿ƒæ—ç™½ã€‚
    æ³¨æ„é€‚å½“æ¢è¡Œä»¥å‡å°‘è¯»è€…çš„é˜…è¯»éš¾åº¦ã€‚åˆ†ä¸‰åˆ°å››æ®µç”Ÿæˆå†…å¿ƒæ—ç™½ã€‚ä¸è¦å†™è¯—ã€‚
    çº¦500å­—ã€‚
    å¿…é¡»ä»¥ç¬¬ä¸€äººç§°å™è¿°ã€‚
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›æ•°æ®ï¼š
    {{
      "inner_monologue": "è¿™é‡Œæ˜¯ç”Ÿæˆçš„å†…å¿ƒæ—ç™½å†…å®¹"
    }}
    """

    try:
        # ç¬¬ä¸€æ¬¡ç”Ÿæˆ
        messages = [{"role": "user", "content": prompt}]
        response = send_messages(messages)
        parsed_response = extract_json(response)
        
        if parsed_response and "inner_monologue" in parsed_response:
            # äºŒæ¬¡æ ¡éªŒ
            check_prompt = f"""
            é’ˆå¯¹ä¸Šä¸€æ¬¡ç”Ÿæˆçš„å†…å¿ƒæ—ç™½ï¼š
            {parsed_response["inner_monologue"]}

            è¯·æ£€æŸ¥å¹¶ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼š
            - ä¿®æ­£æ ‡ç‚¹/ç©ºæ ¼é—®é¢˜
            - æ”¹å–„è¯­å¥é€šé¡ºåº¦
            - ç»Ÿä¸€äººç§°ï¼ˆç¬¬ä¸€äººç§°ï¼‰ï¼Œå¿…é¡»ä»¥ç¬¬ä¸€äººç§°å™è¿°ã€‚
            - åˆ é™¤å¤–è¯­å†…å®¹
            - é˜²æ­¢åœºæ™¯è¿‡äºå…·ä½“
            - ç¡®ä¿500å­—é•¿åº¦
            - åˆ é™¤å¥‡æ€ªæ¯”å–»
            - ä¿®æ­£è¯­ç—…/é”™åˆ«å­—
            
            ç›´æ¥è¿”å›ä¼˜åŒ–åçš„JSONï¼š
            {{
                "inner_monologue": "è¿™é‡Œæ˜¯ä¿®æ”¹åç”Ÿæˆçš„å†…å¿ƒæ—ç™½å†…å®¹"
            }}
            """
            messages.append({"role": "user", "content": check_prompt})
            # ç¬¬äºŒæ¬¡ç”Ÿæˆ
            check_response = send_messages(messages)
            check_parsed_response = extract_json(check_response)
            
            if check_parsed_response and "inner_monologue" in check_parsed_response:
                return index, {
                    "è‡ªæˆ‘è‚¯å®šè¯­": sentence,
                    "è‡ªæˆ‘æ—ç™½": clean_value(check_parsed_response["inner_monologue"]),
                    "MODEL_NAME": MODEL_NAME
                }
            else:
                logging.warning(f"ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {sentence}")
                return index, None
        else:
            logging.warning(f"ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {sentence}")
            return index, None

    except Exception as e:
        logging.error(f"å¤„ç† {sentence} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return index, None

def save_checkpoint(completed_indexes):
    """ä¿å­˜å·²å®Œæˆçš„ä»»åŠ¡ç´¢å¼•"""
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        f.write(",".join(map(str, completed_indexes)))

def load_checkpoint():
    """åŠ è½½å·²å®Œæˆçš„ä»»åŠ¡ç´¢å¼•"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            content = f.read().strip()
            return set(map(int, content.split(","))) if content else set()
    return set()

def process_sentences_concurrently(sentences, start_index=0, ignore_checkpoint=False):
    """
    ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç†å¥å­ã€‚
    å½“ ignore_checkpoint=True æ—¶ï¼Œå¿½ç•¥æ‰€æœ‰å·²ç»å®Œæˆçš„checkpointè®°å½•ã€‚
    """
    results = []
    fail_data = []
    
    if ignore_checkpoint:
        completed_indexes = set()
    else:
        completed_indexes = load_checkpoint()

    logging.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(sentences)} æ¡è‡ªæˆ‘è‚¯å®šè¯­...")

    # ç­›é™¤æ‰å·²ç»å®Œæˆçš„ç´¢å¼•ï¼ˆå½“ ignore_checkpoint=False æ—¶æ‰ä¼šèµ·ä½œç”¨ï¼‰
    to_process = [
        (i + start_index, s)
        for i, s in enumerate(sentences)
        if (i + start_index) not in completed_indexes
    ]
    
    if not to_process:
        logging.info("æ‰€æœ‰æ•°æ®å·²å®Œæˆï¼Œè·³è¿‡å¤„ç†ã€‚")
        return results, fail_data

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_index = {
            executor.submit(process_sentence, idx, sent): idx
            for idx, sent in to_process
        }

        for future in tqdm(as_completed(future_to_index), total=len(future_to_index), desc="ç”Ÿæˆå†…å¿ƒæ—ç™½", unit="æ¡"):
            index, result = future.result()
            if result:
                results.append(result)
                completed_indexes.add(index)
            else:
                # è®°å½•å¤±è´¥æ•°æ®
                original_sentence = sentences[index - start_index]
                fail_data.append({"è‡ªæˆ‘è‚¯å®šè¯­": original_sentence})
    
    # ä¿å­˜æœ€æ–°çš„ checkpointï¼ˆä»…å½“ä¸å¿½ç•¥checkpointæ—¶æ‰ä¿å­˜ï¼‰
    if not ignore_checkpoint:
        save_checkpoint(completed_indexes)

    return results, fail_data

def save_results(results, fail_data):
    """ä¿å­˜æˆåŠŸå’Œå¤±è´¥æ•°æ®"""
    # 1. æˆåŠŸæ•°æ®è¿½åŠ ä¿å­˜
    if results:
        output_df = pd.DataFrame(results)
        if os.path.exists(OUTPUT_FILE):
            existing_df = pd.read_csv(OUTPUT_FILE, encoding="utf-8-sig")
            output_df = pd.concat([existing_df, output_df], ignore_index=True)

        output_df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
        logging.info(f"âœ… ç»“æœå·²è¿½åŠ ä¿å­˜è‡³ {OUTPUT_FILE}")
    else:
        logging.info("æ²¡æœ‰æ–°çš„æˆåŠŸæ•°æ®éœ€è¦ä¿å­˜ã€‚")

    # 2. å¤±è´¥æ•°æ®è¦†ç›–ä¿å­˜
    if fail_data:
        fail_df = pd.DataFrame(fail_data)
        fail_df.to_csv(FAIL_FILE, index=False, encoding="utf-8-sig")
        logging.warning(f"âš ï¸ å¤±è´¥æ•°æ®å·²ä¿å­˜è‡³ {FAIL_FILE}")
    else:
        # è‹¥æ— å¤±è´¥æ•°æ®ï¼Œåˆ™æ¸…ç†ä¹‹å‰çš„ fail_data.csv
        if os.path.exists(FAIL_FILE):
            os.remove(FAIL_FILE)
        logging.info("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†æˆåŠŸï¼Œæ— å¤±è´¥æ•°æ®")

def main():
    """ä¸»å…¥å£ï¼šç”¨æˆ·é€‰æ‹©è¿è¡Œæ¨¡å¼"""
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("0. EXIT")
    print("1. ä»æ€»æ•°æ®ä¸­å¼€å§‹ç”Ÿäº§")
    print("2. é‡è¯•å¤±è´¥çš„æ¡ˆä¾‹ï¼ˆè¯»å– fail_data.csvï¼‰")
    print("3. ä» checkpoint ä¸­æ¢å¤ç”Ÿäº§")
    choice = input("è¯·è¾“å…¥ 0, 1, 2 æˆ– 3: ").strip()

    if choice == "1":
        # è¯»å–æ•´ä»½æ•°æ®
        # df = pd.read_csv("0315å¥å­æ›´æ–° - æ±‡æ€»è¡¨.csv", encoding="utf-8-sig").sample(100)
        df = pd.read_csv("0315å¥å­æ›´æ–° - æ±‡æ€»è¡¨.csv", encoding="utf-8-sig")
        df = df[df["æƒé‡"] == 3]
        df.to_csv(CACHE_FILE, index=False, encoding="utf-8-sig")  # ä¿å­˜åˆ° cache ä»¥ä¾¿æ–­ç‚¹ç»­ä¼ 
        start_index = 0
        sentences = df['è‡ªæˆ‘è‚¯å®šè¯­'].dropna().tolist()
        print(f"len(sentences): {len(sentences)}")
        # ä½¿ç”¨å¹¶å‘å¤„ç†æ•°æ®ï¼ˆæ­¤æ—¶ä¸å¿½ç•¥checkpointï¼Œä»¥ä¾¿å¯ä»¥ä»ä¸­æ–­å¤„ç»§ç»­ï¼‰
        results, fail_data = process_sentences_concurrently(sentences, start_index, ignore_checkpoint=False)
        save_results(results, fail_data)

    elif choice == "2":
        # é‡è¯•å¤±è´¥çš„æ¡ˆä¾‹
        if os.path.exists(FAIL_FILE):
            df = pd.read_csv(FAIL_FILE, encoding="utf-8-sig")
            sentences = df['è‡ªæˆ‘è‚¯å®šè¯­'].dropna().tolist()
            start_index = 0
            # å…³é”®ï¼šå¿½ç•¥ checkpointï¼Œç¡®ä¿å¯ä»¥é‡æ–°å¤„ç†
            results, fail_data = process_sentences_concurrently(sentences, start_index, ignore_checkpoint=True)
            save_results(results, fail_data)
        else:
            print("âŒ æ²¡æœ‰ fail_data.csvï¼Œæ‰€æœ‰æ•°æ®å·²æˆåŠŸå¤„ç†ï¼")

    elif choice == "3":
        # ä» checkpoint ä¸­æ¢å¤
        if os.path.exists(CACHE_FILE):
            df = pd.read_csv(CACHE_FILE, encoding="utf-8-sig")
            completed_indexes = load_checkpoint()
            if len(completed_indexes) >= len(df):
                print("ğŸ‰ æ‰€æœ‰æ•°æ®å·²æˆåŠŸå¤„ç†ï¼Œæ— éœ€æ¢å¤")
                return
            start_index = 0
            sentences = df['è‡ªæˆ‘è‚¯å®šè¯­'].dropna().tolist()
            results, fail_data = process_sentences_concurrently(sentences, start_index, ignore_checkpoint=False)
            save_results(results, fail_data)
        else:
            print("âŒ æœªæ‰¾åˆ° cache.csvï¼Œæ— æ³•ä» checkpoint æ¢å¤ï¼")

    elif choice == "0":
        exit(1)
    else:
        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 0, 1, 2 æˆ– 3")

if __name__ == "__main__":
    main()